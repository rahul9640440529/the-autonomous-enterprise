# Reasoning Traces

> This document supports **Chapter 6** of *The Autonomous Enterprise*. The book, included as a PDF in the root of this repository, is the authoritative source of truth. See [`BOOK_CONTEXT.md`](../BOOK_CONTEXT.md) for scope and constraints.

---

## 1. Why Explainability Is an Engineering Requirement

Autonomous agents make decisions and take actions. When those decisions affect customers, systems, or business outcomes, the organization must be able to answer: What did the agent decide? Why did it decide that? What information did it use?

These are not philosophical questions. They are operational requirements.

**Incident response**: When something goes wrong, investigators need to understand what happened. They cannot remediate a failure they cannot trace. They cannot prevent recurrence if they cannot identify the cause.

**Accountability**: Organizations are accountable for the actions of their agents. Regulators, auditors, and customers may require explanations. "The model decided" is not an acceptable answer.

**Quality assurance**: Agents may produce correct outputs for incorrect reasons. Without visibility into reasoning, correct behavior cannot be distinguished from coincidental behavior. Quality assurance requires evidence of sound reasoning, not just acceptable outcomes.

**Trust calibration**: Operators need to understand when to trust agent decisions and when to intervene. Explainability enables informed oversight. Without it, operators either over-trust or under-trust, both of which create risk.

Explainability is not a feature for end users. It is an engineering requirement for the operators, reviewers, and auditors who must understand and govern agent behavior.

---

## 2. What a Reasoning Trace Is

A reasoning trace is a structured record of how an agent arrived at a decision or action.

The trace captures:

**Inputs considered**: What information did the agent have access to when it made the decision? This includes user intent, retrieved context, memory, and any constraints provided by the environment.

**Constraints applied**: What rules, policies, or boundaries influenced the decision? This includes guardrails, scope limitations, and organizational policies the agent was aware of.

**Reasoning steps**: What intermediate conclusions did the agent reach? How did it decompose the problem? What options did it consider? What tradeoffs did it evaluate?

**Decision made**: What did the agent conclude? What action did it propose or take? What was the stated justification?

**Outcome observed**: What happened after the action was taken? Did the result match expectations? If not, how did the agent respond?

A reasoning trace is not a transcript of everything the agent processed. It is a curated record that captures the decision-relevant elements in a form that humans can review.

The trace is structured. It follows a defined schema that supports consistent interpretation across agents, tasks, and time. Unstructured explanations are difficult to compare, analyze, or audit at scale.

---

## 3. What a Reasoning Trace Is Not

Reasoning traces must be distinguished from other forms of agent output that may appear similar but serve different purposes.

**Not raw chain-of-thought**: Chain-of-thought is a prompting technique that encourages models to generate intermediate reasoning steps. The output is unstructured text optimized for model performance, not human review. It may contain artifacts, false starts, or reasoning patterns that are meaningful to the model but confusing or misleading to humans.

**Not token streams**: The sequence of tokens generated by a model is an implementation artifact. It reflects how the model produces output, not what the agent decided or why. Token-level visibility does not provide meaningful explainability.

**Not model internals**: Attention weights, activation patterns, and embedding representations are research tools for understanding model behavior. They are not explanations that operators or auditors can act on. They require specialized expertise to interpret and do not map cleanly to decisions or actions.

**Not justifications generated after the fact**: A reasoning trace is captured during decision-making, not reconstructed afterward. Post-hoc rationalizations are unreliable because they may not reflect what actually influenced the decision.

Exposing raw chain-of-thought or model internals creates risks:

- It may reveal information the organization does not want disclosed
- It may be misinterpreted by readers who lack context
- It may create false confidence in explanations that do not reflect true causes
- It may expose attack surfaces for prompt injection or manipulation

Reasoning traces are designed artifacts. They capture what matters for review and accountability in a form that is safe and useful.

---

## 4. Trace Granularity and Scope

A reasoning trace must be scoped appropriately. Traces that are too fine-grained or too coarse undermine their purpose.

**Overly fine-grained traces** capture every micro-decision, every context lookup, every intermediate step. They produce voluminous records that are expensive to store, difficult to review, and impractical for audit. Reviewers cannot find the signal in the noise. Fine-grained traces may also expose information that should remain internal.

**Overly coarse traces** capture only the final decision and a summary justification. They omit the intermediate reasoning that makes the decision understandable. When something goes wrong, coarse traces do not provide enough information to diagnose the cause.

The appropriate granularity depends on the nature of the decision:

**Task-level traces** capture the reasoning for a complete task—from initial goal to final outcome. They show how the agent decomposed the problem, what steps it took, and how it handled obstacles. Task-level traces are appropriate for complex, multi-step work.

**Decision-level traces** capture the reasoning for a single significant decision within a task. They show what options were considered, what constraints applied, and why the chosen option was selected. Decision-level traces are appropriate for high-stakes choices.

**Action-level traces** capture the reasoning for a specific action, particularly actions with significant consequences. They show what prompted the action, what the agent expected to happen, and what preconditions were verified.

Traces should be scoped to units of accountability. If a human would ask "why did the agent do that?", the trace should provide an answer at that level of abstraction.

---

## 5. Using Traces for Review and Accountability

Reasoning traces enable several forms of oversight without requiring access to the underlying model.

**Human review**: Operators, supervisors, or domain experts can review traces to assess whether agent decisions were reasonable. They can identify errors in reasoning, gaps in context, or misapplication of constraints. Review can occur in real time or after the fact.

**Incident analysis**: When an agent produces an unexpected or harmful outcome, investigators can examine the trace to understand the sequence of reasoning that led there. They can identify where the reasoning went wrong—was the input flawed? Was a constraint missing? Did the agent misinterpret the goal?

**Pattern detection**: Traces can be analyzed in aggregate to identify recurring issues. If multiple traces show similar reasoning errors, the underlying cause may be a systemic problem—a flawed prompt, a missing guardrail, or a gap in context.

**Accountability demonstration**: When stakeholders require evidence that decisions were made appropriately, traces provide that evidence. Auditors can verify that constraints were applied, that inputs were considered, and that reasoning followed expected patterns.

**Training and improvement**: Traces provide examples of agent reasoning that can inform improvements. Engineers can identify cases where reasoning was suboptimal and adjust prompts, context, or constraints accordingly.

Reasoning traces make agent behavior legible. They translate opaque model behavior into structured evidence that humans can assess, question, and act upon.

---

## 6. Boundaries Introduced Here

This document establishes reasoning traces as a first-class architectural concept. It defines what traces are, what they capture, and how they support explainability.

**What this document defines**:

- Reasoning traces as structured records of decision-making
- The elements that traces must capture: inputs, constraints, reasoning, decisions, outcomes
- The distinction between traces and raw chain-of-thought, token streams, or model internals
- Appropriate granularity for traces: task-level, decision-level, action-level
- How traces support review, incident analysis, and accountability

**What this document does not define**:

- How traces are stored, indexed, or retrieved
- Retention policies for trace records
- Compliance requirements that traces must satisfy
- Access controls for trace data
- Integration with audit infrastructure or logging systems

Storage, retention, compliance, and infrastructure are addressed in subsequent chapters of the book. This document establishes only the conceptual foundation: what reasoning traces are, why they matter, and how they should be structured.

---

## References

- *The Autonomous Enterprise*, Chapter 6 — Full treatment of reasoning traces and explainability architecture.
- [`agent-control-loop.md`](./agent-control-loop.md) — The control loop within which reasoning occurs and traces are generated.
- [`guardrails.md`](./guardrails.md) — Constraints that may appear in reasoning traces as applied rules.
- [`BOOK_CONTEXT.md`](../BOOK_CONTEXT.md) — Repository scope, constraints, and authorial intent.
